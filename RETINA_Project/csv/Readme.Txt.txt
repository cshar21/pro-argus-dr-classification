..in this case data augmentation was not applied during pre processing 
only clahe 

resizing was not applied here to give you the option to switch to other models if you want to

data augmentation needs to be applied during training because :

Static Augmentations: The model will see the exact same augmented version of an image every epoch ‚Üí defeats the purpose of augmentation (no randomness).

Apply augmentations dynamically during training via Albumentations in the Dataset class ‚Üí new randomness every epoch.



ISSUES :
1. Augmentations Should Happen During Training, Not Preprocessing 
   Changes : Remove all augmentations from get_augmentations() in preprocessing.
Only apply augmentations dynamically during training 

2. Aggressive Spatial Transforms Might Distort Retinal Lesions 
   need to reconfirm the correct values for augmentation later

3. Resize Should Happen During Training, Not Preprocessing 
   to be able to switch to other models later without pre processing again

4. Normalization Issue in Preprocessing
üî¥ Issue:

Right now, ToTensorV2() in preprocessing converts images to [0,1] range, but later you multiply by 255 to save them as PNGs.
This might cause floating-point conversion artifacts
‚úÖ Fix:

Do not use ToTensorV2() in preprocessing.
Save images as uint8 without normalization.
Normalize dynamically in your DataLoader during training.

5. Check train/val split to avoid data leakage. 
   even though i have APTOS test images and csv why do a train / val split ?
   üîç Why Validation is Still Important
1Ô∏è‚É£ Prevents Overfitting:

Without validation, you have no way to monitor when your model is overfitting on the training data.
Loss decreasing ‚â† better model! Sometimes, the model memorizes instead of generalizing.
2Ô∏è‚É£ Helps Tune Hyperparameters:

Learning rate schedules, augmentation strategies, and early stopping all depend on a validation set.
3Ô∏è‚É£ Avoids Surprises on APTOS Test Set:

If your model fails on the APTOS test set, you won‚Äôt know why (bad training? overfitting?).
A validation set lets you debug issues before the final test.
‚úÖ Option 1 (Recommended): Split Train/Val Within Each Dataset

Take 10-20% of each dataset (APTOS, EyePACS, Messidor) for validation.
Use stratified split (so all severity levels are balanced).
Train on remaining 80-90%.
‚úÖ Option 2 (Alternative): Use One Dataset for Validation

Train on EyePACS + Messidor + 90% APTOS
Validate on 10% of APTOS
Test on APTOS Test Set
‚úÖ Option 3 (Risky): No Validation, Just Train & Test on APTOS
‚ö† This means no early stopping, no fine-tuning, no debugging.
‚ö† If the model performs badly on APTOS, you won‚Äôt know what went wrong.





Create train-validation split dynamically when loading data during training.
Use Stratified Split (train_test_split(stratify=y))
OR Messidor as validation
OR K-Fold Cross-Validation




üìù What You Need to Do
1Ô∏è‚É£ Modify Preprocessing Script

Remove all augmentations except CLAHE
Do not resize (keep original image size)
Do not use ToTensorV2() in preprocessing
Save images as uint8 PNGs (not float)
2Ô∏è‚É£ Modify Training Dataset

Apply Albumentations during training inside __getitem__()
Dynamically resize inside the dataset
Normalize inside the dataset


